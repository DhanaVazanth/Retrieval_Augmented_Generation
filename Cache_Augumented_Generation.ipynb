{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://media.licdn.com/dms/image/v2/D5622AQEgdGokK9B4QQ/feedshare-shrink_800/B56ZRpOiggGQAk-/0/1736932208572?e=2147483647&v=beta&t=GdR_RfRuudlFrspbx5MLaWGn9cDkuUVZE-BaS8EDem8\" alt=\"Http cat\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache-Augmented Generation (CAG)\n",
    "--------------------------------\n",
    "CAG enhances LLM responses by storing and reusing previous query-response pairs in a cache to:\n",
    "\n",
    "-   ‚úî Reduce redundant computations\n",
    "-   ‚úî Improve response time\n",
    "-   ‚úî Enhance consistency\n",
    "-   ‚úî Lower API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ† How Does CAG Work?\n",
    "----------------------\n",
    "\n",
    "-   Cache Lookup: Before generating, the model checks if the query exists in the cache.\n",
    "\n",
    "-   Cache Hit ‚Üí Returns a stored response immediately.\n",
    "\n",
    "-   Cache Miss ‚Üí Calls the LLM, generates a response, and stores it in the cache for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Advanced CAG Techniques\n",
    "----------------------------\n",
    "-   üîπ Hybrid CAG-RAG: Use cache first, then fall back on retrieval.\n",
    "-   üîπ LRU Cache: Remove least-used entries to prevent memory overflow.\n",
    "-   üîπ Distributed Caching: Store responses across multiple nodes for scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature               | CAG (Cache-Augmented Generation)         | RAG (Retrieval-Augmented Generation)   |\n",
    "|----------------------|----------------------------------|----------------------------------|\n",
    "| **Core Mechanism**   | Uses caching to store and reuse past responses | Dynamically retrieves documents from an external knowledge base |\n",
    "| **Response Time**    | Faster, as cached responses are instantly retrieved | Slower, due to real-time retrieval and processing |\n",
    "| **Knowledge Updates** | Limited; requires cache updates for new information | Dynamically retrieves the latest information |\n",
    "| **Storage**          | Requires local storage for caching responses | Requires storage for vector embeddings (FAISS, ChromaDB, etc.) |\n",
    "| **Use Case**         | Best for queries with repetitive patterns | Best for queries requiring real-time knowledge retrieval |\n",
    "| **Computational Cost** | Lower, as it avoids frequent LLM calls | Higher, as each query involves retrieval and generation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIRED LIBRARIES\n",
    "--------------------\n",
    "\n",
    "-   PyPDF ‚Üí To extract text from PDFs\n",
    "\n",
    "-   Langchain ‚Üí Framework to build the CAG pipeline\n",
    "\n",
    "-   FAISS ‚Üí Vector database for similarity search\n",
    "\n",
    "-   Ollama LLM ‚Üí Open-source model for answering queries\n",
    "\n",
    "-   Pickle ‚Üí To store and retrieve cached responses efficiently\n",
    "\n",
    "-   Hashlib ‚Üí For generating hash-based cache keys\n",
    "\n",
    "\n",
    "PACKAGE INSTALLATION \n",
    "-----------------------\n",
    "```bash\n",
    "pip install PyPDF\n",
    "pip install -U langchain-community\n",
    "pip install faiss-cpu\n",
    "pip install -U langchain_ollama\n",
    "pip install pickle-mixin\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize cache (dictionary-based storage)\n",
    "cache = {}\n",
    "CACHE_FILE = \"cache.pkl\"\n",
    "\n",
    "# Load existing cache if available\n",
    "def load_cache():\n",
    "    global cache\n",
    "    try:\n",
    "        with open(CACHE_FILE, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        cache = {}\n",
    "\n",
    "# Save cache to a file\n",
    "def save_cache():\n",
    "    with open(CACHE_FILE, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "load_cache()\n",
    "\n",
    "# Load and process PDF data\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    return docs\n",
    "\n",
    "# Create embeddings and FAISS vector store\n",
    "def create_vectorstore(docs):\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Generate response with caching\n",
    "def generate_response(query, vectorstore):\n",
    "    query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    if query_hash in cache:\n",
    "        print(\"‚úÖ Cache Hit! Returning stored response.\")\n",
    "        return cache[query_hash]\n",
    "    \n",
    "    print(\"‚ùå Cache Miss. Retrieving relevant context and generating response...\")\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    context_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    llm = OllamaLLM(model=\"llama3.2\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Use the following context to answer the query:\n",
    "        Context: {context}\n",
    "        Query: {query}\n",
    "        Response: \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"query\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    cache[query_hash] = response\n",
    "    save_cache()\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\santh\\OneDrive\\Desktop\\MDTM37\\python_assignment (1).pdf\"  # Update with your PDF path\n",
    "docs = load_pdf(pdf_path)\n",
    "vectorstore = create_vectorstore(docs)\n",
    "query = \"What is the main topic of the document?\"\n",
    "print(generate_response(query, vectorstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of the document?\"\n",
    "print(generate_response(query, vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic ?\"\n",
    "print(generate_response(query, vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic ?\"\n",
    "print(generate_response(query, vectorstore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
